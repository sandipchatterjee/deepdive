#!/usr/bin/env bash
# torque/compute-execute -- Executes a process remotely using the Torque scheduler
# $ compute-execute input_sql=... command=... output_relation=...
#
# To limit the number of parallel processes, set the DEEPDIVE_NUM_PROCESSES
# environment or the 'deepdive.computers.local.num_processes' in
# computers.conf:
# $ export DEEPDIVE_NUM_PROCESSES=2
# $ compute-execute input_sql=... command=... output_relation=...
##
set -euo pipefail

: ${DEEPDIVE_PREFIX_TABLE_TEMPORARY:=dd_tmp_} ${DEEPDIVE_PREFIX_TABLE_OLD:=dd_old_}

# load compute configuration
eval "$(jq2sh <<<"$DEEPDIVE_COMPUTER_CONFIG" \
    num_processes='.num_processes' \
    ssh_user='.ssh_user' \
    ssh_host='.ssh_host' \
    remote_home='.remote_home' \
    poll_period_secs='.poll_period_secs' \
    #
)"
# respect the DEEPDIVE_NUM_PROCESSES environment
num_processes=${DEEPDIVE_NUM_PROCESSES:-${num_processes:-$(
        # detect number of processor cores
        nproc=$(
            # Linux typically has coreutils which includes nproc
            nproc ||
            # OS X
            sysctl -n hw.ncpu ||
            # fall back to 1
            echo 1
        )
        if [[ $nproc -gt 1 ]]; then
            # leave one processor out
            let nproc-=1
        elif [[ $nproc -lt 1 ]]; then
            nproc=1
        fi
        echo $nproc
    )}}

# declare all input arguments
declare -- "$@"

ssh_info="${ssh_user}@${ssh_host}"
REMOTE_DEEPDIVE_APP="$remote_home/$(basename $DEEPDIVE_APP)"
REMOTE_CWD="$REMOTE_DEEPDIVE_APP/run/$DEEPDIVE_CURRENT_PROCESS_NAME"
REMOTE_SH_DIR="$REMOTE_CWD/remote.sh"
REMOTE_IN_DIR="$REMOTE_CWD/remote.in"
REMOTE_OUT_DIR="$REMOTE_CWD/remote.out"
REMOTE_ERR_DIR="$REMOTE_CWD/remote.err"

# show configuration
echo "Executing with the following configuration:"
echo " num_processes=$num_processes"
echo " ssh_user=$ssh_user"
echo " ssh_host=$ssh_host"
echo " remote_home=$remote_home"
echo " poll_period_secs=$poll_period_secs"

echo "DEEPDIVE_APP = $DEEPDIVE_APP"
echo "REMOTE_DEEPDIVE_APP = $remote_home/$(basename $DEEPDIVE_APP)"
echo "DEEPDIVE_CURRENT_PROCESS_NAME = $DEEPDIVE_CURRENT_PROCESS_NAME"

# Copy application folder to remote shared directory.
echo "Copying deepdive app to remote node"
rsync -aH $DEEPDIVE_APP $ssh_info:$remote_home

# Also ensure that REMOTE_CWD exists in remote shared directory. For some
# reason, without the command below,
# process/grounding/factor/inf_istrue_has_spouse fails since the directory
# doesn't exist. 
ssh $ssh_info "mkdir -p $REMOTE_CWD"

set -x

# Prepare submission script
# XXX there are conditional branches below depending on whether input_sql
# and/or output_relation is given, to support four use cases. Depending on
# the cases, need to generate various submission scripts. 
# 1) executing command while streaming data from/to the database
# 2) input-only command which has no output to the database and streams from the database
# 3) output-only command which has no input from the database and streams to the database
# 4) database-independent command which simply runs in parallel

# We can use the environment variable PBS_ARRAYID when submitting this job as a job 
# array.
echo "Preparing submission script at remote node"
{
    echo "#PBS -N $DEEPDIVE_CURRENT_PROCESS_NAME"
    echo "#PBS -o $REMOTE_OUT_DIR"
    echo "#PBS -e $REMOTE_ERR_DIR"
    echo
    echo "export DEEPDIVE_APP=\"$REMOTE_DEEPDIVE_APP\""
    echo '"$SHELL"'" -c \"$command\" < \"$REMOTE_IN_DIR\""
} | 
ssh $ssh_info "cat > $REMOTE_SH_DIR"

# Prepare input data
if [[ -n $input_sql ]]; then
    deepdive-sql eval "$input_sql" format="$DEEPDIVE_LOAD_FORMAT" |
    ssh $ssh_info "cat > $REMOTE_IN_DIR"
fi

# Submit job to remote node. 
compute_submit() {
  # XXX: These are the correct commands, but need workaround for Macs due to 
  # Kerberos issue. 
  #ssh $ssh_info "qsub -V $REMOTE_DEEPDIVE_APP/run/$DEEPDIVE_CURRENT_PROCESS_NAME/remote.sh"

  # XXX: Right now, using this script to allow qsub to run, however this will
  # prompt user for password in the middle of the script, not elegant. 
  ssh $ssh_info "kinit -r 30d; qsub -V $REMOTE_SH_DIR" 
}

# Poll for status. Should return I for incomplete, S for success, or F for fail.
compute_status() {
  ssh $ssh_info "qstat -f -1 $JOB_ID | grep job_state | sed 's/.*job_state = //'"
}

# Submit job. 
# This is a hack that supposedly prints out the output of ssh asking for the 
# password. But the output seems to not be flushed to stdout. 
exec 5>&1
JOB_ID="$(compute_submit | tee >(cat - >&5) | grep $ssh_host)"
echo "Job ID: '$JOB_ID'"
if [[ "$JOB_ID" != *"$ssh_host" ]]; then
  echo "Job submission failed with the following error:"
  printf "$JOB_ID"
  error
fi
echo "Job submitted"

echo -n "Waiting for job id $JOB_ID to complete"
while [[ "$(compute_status)" != "S" ]] && ssh -q $ssh_info "[[ ! -f $REMOTE_OUT_DIR ]]"; do
  echo -n "."
  sleep $poll_period_secs
done
echo "done!"

# prepare a temporary output table when output_relation is given
if [[ -n $output_relation ]]; then
    # some derived values
    output_relation_tmp="${DEEPDIVE_PREFIX_TABLE_TEMPORARY}${output_relation}"

    # show configuration
    echo " output_relation_tmp=$output_relation_tmp"
    echo

    # use an empty temporary table as a sink instead of TRUNCATE'ing the output_relation
    deepdive-create table-if-not-exists "$output_relation"
    db-create-table-like "$output_relation_tmp" "$output_relation"

    # Stream output from remote submission node directly to stdin, and to 
    # deepdive load
    ssh $ssh_info "cat $REMOTE_OUT_DIR" > /dev/stdout |

    # use mkmimo again to merge outputs of multiple processes into a single stream
    #mkmimo process-*.output \> /dev/stdout |

    # load the output data to the temporary table in the database
    # XXX hiding default progress bar from deepdive-load
    # TODO abbreviate this env into a show_progress option, e.g., recursive=false
    # TODO I am really unsure as to how to show the progress here. This might work
    # As opposed to local, this should be a blocking process, since we don't use
    # mkmimo.
    show_progress input_to "$DEEPDIVE_CURRENT_PROCESS_NAME output" -- \
    env DEEPDIVE_PROGRESS_FD=2 \
    deepdive-load "$output_relation_tmp" /dev/stdin

    # rename the new temporary table
    # TODO maybe use PostgreSQL's schema support here?
    echo "Replacing $output_relation with $output_relation_tmp"
    output_relation_old="${DEEPDIVE_PREFIX_TABLE_OLD}${output_relation}"
    deepdive-sql "DROP TABLE IF EXISTS ${output_relation_old};" || true
    deepdive-sql "ALTER TABLE ${output_relation}     RENAME TO ${output_relation_old};"
    deepdive-sql "ALTER TABLE ${output_relation_tmp} RENAME TO ${output_relation};"
    deepdive-sql "DROP TABLE IF EXISTS ${output_relation_old};" || true
    # and analyze the table to speed up future queries
    db-analyze "${output_relation}"
fi
